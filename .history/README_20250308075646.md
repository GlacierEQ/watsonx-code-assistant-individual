# License
- The license for IBM watsonx Code Assistant Individual can be found in the [product-licenses](./product-licenses/) folder in this repository.

- You can use this repository to add issues for watsonx Code Assistant Individual. The license for issues, discussion, and any files or samples shared in issues can be found in the [LICENSE](./LICENSE) file.


# IBM watsonx Code Assistant Individual

## Features

Watsonx Code Assistant Individual is an innovative, lightweight AI coding companion built for IBM’s state-of-the-art Granite large language models. This companion offers robust, contextually aware AI coding assistance for popular programming languages such as C, C++, Go, Java, JavaScript, Python, and TypeScript. Seamlessly integrated into Visual Studio Code, watsonx Code Assistant Individual accelerates development productivity and simplifies coding tasks by providing powerful AI support hosted locally on the developer’s laptop or workstation using Ollama.

### Chat with code models

- Chat with an <a href="https://www.ibm.com/granite" target="_blank">IBM Granite</a> code model to create code, and ask general programming questions.
- Use the chat to explain and extend existing code from your workspace.

![explain](https://github.com/ibm-granite/watsonx-code-assistant-individual/raw/HEAD/images/Explain.png)

### Code completion

Complete the line that you're currently typing:

<img src="https://github.com/ibm-granite/watsonx-code-assistant-individual/raw/HEAD/images/Single-line.gif" height=200 alt="Single-line completion in watsonx Code Assistant Individual">

And even full methods and functions:

<img src="https://github.com/ibm-granite/watsonx-code-assistant-individual/raw/HEAD/images/Multi-line.gif" height=350 alt="Multi-line completion in watsonx Code Assistant Individual">

### Turn comments into code

Create a comment that describes a function, method, or piece of logic in your editor, and have watsonx Code Assistant Individual create it.

<img src="https://github.com/ibm-granite/watsonx-code-assistant-individual/raw/HEAD/images/comment-to-code.gif" height=200 alt="Comment to code generation in watsonx Code Assistant Individual">

### Everything is local, configurable by you

- Install the granite 8b code instruct model for both chat and code completion.
- Optionally, install the granite 8b code base model for code completion.
- These models run locally on your laptop using <a href="https://ollama.com" target="_blank">Ollama</a>.

## Setup

Watsonx Code Assistant Individual accesses models through <a href="https://ollama.com" target="_blank">Ollama</a>, which is a widely used local inferencing engine for LLMs. Ollama wraps the underlying model-serving project <a href="https://github.com/ggml-org/llama.cpp" target="_blank">llama.cpp</a>.

### Automated Installation (Linux)

For Linux users, you can use the following automated installation script that installs all necessary components including system dependencies, Python environment, Visual Studio Code, Ollama, and GPU/NPU optimizations:

```bash
#!/bin/bash
# Optimized Installation Script for IBM Watsonx Code Assistant Individual
# Ensures GPU/NPU acceleration, memory optimization, and efficient AI deployment

set -e  # Exit on error

### STEP 1: SYSTEM UPDATE & DEPENDENCIES ###
echo "🔹 Updating system packages..."
sudo apt update && sudo apt upgrade -y  # For Debian-based systems

### STEP 2: INSTALL PYTHON & VIRTUAL ENVIRONMENT ###
echo "🔹 Installing Python & Virtual Environment..."
sudo apt install -y python3 python3-venv python3-pip
python3 -m venv watsonx_env
source watsonx_env/bin/activate

### STEP 3: INSTALL VS CODE ###
echo "🔹 Installing Visual Studio Code..."
wget -qO- https://packages.microsoft.com/keys/microsoft.asc | gpg --dearmor > packages.microsoft.gpg
sudo install -o root -g root -m 644 packages.microsoft.gpg /etc/apt/trusted.gpg.d/
echo "deb [arch=amd64] https://packages.microsoft.com/repos/code stable main" | sudo tee /etc/apt/sources.list.d/vscode.list
sudo apt update
sudo apt install -y code

### STEP 4: INSTALL OLLAMA (AI MODEL HOSTING) ###
echo "🔹 Installing Ollama for local AI hosting..."
curl -fsSL https://ollama.ai/install.sh | sh

### STEP 5: INSTALL CUDA/NPU DRIVERS & TENSOR OPTIMIZATION ###
echo "🔹 Installing CUDA & TensorFlow/PyTorch optimizations..."
sudo apt install -y nvidia-cuda-toolkit
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
pip install tensorflow-gpu

### STEP 6: CLONE & SET UP WATSONX CODE ASSISTANT ###
echo "🔹 Cloning Watsonx Code Assistant Repository..."
git clone https://github.com/IBM/watsonx-code-assistant-individual.git
cd watsonx-code-assistant-individual
pip install -r requirements.txt

### STEP 7: ENABLE GPU/NPU PRIORITY ###
echo "🔹 Configuring GPU/NPU as primary resource..."
export CUDA_VISIBLE_DEVICES=0
export TF_GPU_ALLOCATOR=cuda_malloc_async

### STEP 8: FINAL SETUP & LAUNCH ###
echo "✅ Installation complete! Launching Watsonx Code Assistant in VS Code..."
code .
```

Note: After running this script, you'll still need to install the Granite code model and VS Code extension as described in the sections below.

### Install Ollama (AI Model Hosting)

MacOS, Linux, Windows: Download and run the <a href="https://ollama.com/download" target="_blank">ollama installer</a>

### If already installed, update Ollama to the latest stable version
#### On MacOS, If you have installed Ollama via brew, run the command to upgrade:
```
brew upgrade ollama
```

#### If you have installed Ollama via package installer:
Ollama on macOS and Windows will automatically download updates. Click on the taskbar or menubar item, and then click "Restart to update" to apply the update. Updates can also be installed by downloading the latest version manually.

#### On Linux, re-run the install script:
```
curl -fsSL https://ollama.com/install.sh | sh
```

### Start the Ollama inference server

In a terminal window, run:

```shell
ollama serve
```

Leave that window open while you use Ollama.

If you receive the message `Error: listen tcp 127.0.0.1:11434: bind: address already in use`, the Ollama server is already started. There's nothing else that you need to do.

### Install the Granite code model (for AI coding assistance)

Get started with watsonx Code Assistant Individual by installing the `granite-code:8b` model available in the <a href="https://ollama.com/library/granite-code" target="_blank">Ollama library</a>.

1. Open a new terminal window.
2. On the command line, type `ollama run granite-code:8b` to download and deploy the model. You see output similar to the following example:

   ```shell
   pulling manifest 
   pulling 8718ec280572... 100% ▕███████████████████████ 4.6 GB
   pulling e50df8490144... 100% ▕███████████████████████ ▏  123 B
   pulling 58d1e17ffe51... 100% ▕███████████████████████▏  11 KB
   pulling 9893bb2c2917... 100% ▕███████████████████████▏  108 B
   pulling 0e851433eda0... 100% ▕███████████████████████▏  485 B
   verifying sha256 digest 
   writing manifest 
   removing any unused layers 
   success 
   >>> 
   ```

3. Type `/bye` after the `>>>`to exit the Ollama command shell.
4. Try the model by typing:

   ```shell
   ollama run granite-code:8b "How do I create a python class?"
   ```

5. You should see a response similar to the following:

   ```shell
   To create a Python class, you can define a new class using the "class" keyword followed by the name of the class and a colon. Inside the class definition, you can specify the methods and attributes that the class will have. Here is an example: ...
   ```

### Install the watsonx Code Assistant Individual Visual Studio Code extension

This setup is not available for the Eclipse IDE plug-in. It is only available with the Visual Studio Code extension.

1. Open the [watsonx Code Assistant](https://marketplace.visualstudio.com/items?itemName=IBM.wca-core&ssr=false#overview) extension page in the Visual Studio Marketplace.
2. Click **Install** on the Marketplace page.
3. In Visual Studio Code, click **Install** on the extension.

### Configure the Ollama host (if using a different IP or port)

By default, the Ollama server runs on IP address `127.0.0.1`, port `11434`, using http as a protocol. If you change the IP address or the port where Ollama is available:

1. Open the extension settings.
2. Locate the entry for _API Host_.
3. Add the host IP and port.

### Configure the Granite models to use

By default, watsonx Code Assistant Individual uses the `granite-code:8b` model for both chat and code completion.
If your environment has capacity, install the `granite-code:8b-base` model, and use it as _Local Code Gen Model_ as follows.
To use a different model:

1. Install the `granite-code:8b-base` model. See [Install the Granite code model](#install-the-granite-code-model).
2. Open the extension settings.
3. Update the model name for either _Local Code Gen Model_ to `granite-code:8b-base`.

### Securing your setup (Best Practices)

#### Your Visual Studio Code environment

Watsonx Code Assistant Individual does not provide additional security controls. We recommended the following steps to properly secure your setup:

- Apply all Visual Studio Code updates to help ensure that you have the latest security and bug fixes. For more information, see the <a href="https://code.visualstudio.com/docs/setup/setup-overview" target="_blank">Microsoft Documentation</a>.
- The watsonx Code Assistant Individual extension logs are stored in *.log files under `<your home directory>/.wca`. These files are not encrypted, besides the encryption that your file system provides. Safeguard the logs against improper access.

#### Connecting watsonx Code Assistant Individual and Ollama

By default, the Ollama server runs on IP address 127.0.0.1, port 11434, using http as a protocol, on your local device. To use https instead, or go through a proxy server, follow the <a href="https://github.com/ollama/ollama/blob/main/docs/faq.md#how-can-i-use-ollama-with-a-proxy-server" target="_blank">Ollama documentation</a>.

#### Chat conversation storage

Watsonx Code Assistant Individual stores all your chat conversations locally in your file system under `<your home directory>/.wca/chat.db`, in a database format defined by <a href="https://www.sqlite.org/index.html" target="_blank">SQLite</a>. Watsonx Code Assistant Individual does _not_ share conversations with anyone. This file is not encrypted, besides the encryption that your file system provides. Safeguard this file against improper access.

#### Telemetry data

Watsonx Code Assistant Individual does _not_ collect any telemetry data. In general, watsonx Code Assistant Individual does not send any data that it processes to a third party, IBM included.

## Using chat with the Granite code model

### Starting the chat

1. Open the watsonx Code Assistant Individual view by selecting _View -> Open View -> watsonx Code Assistant_ in the menu, or clicking the _watsonx Code Assistant_ icon in the sidebar <img src="https://github.com/ibm-granite/watsonx-code-assistant-individual/raw/HEAD/images/wca-portfolio.svg" height=20 alt="watsonx Code Assistant icon">
2. The chat panel opens to the left of the Visual Studio Code editor.
3. To move the chat, drag the icon to the right or bottom of the editor.

### Interacting with the chat

#### Use natural language

Enter a free-text question or instruction and click **Enter**. watsonx Code Assistant Individual sends your input to the code model, and shows the response in the chat.  

<img src="https://github.com/ibm-granite/watsonx-code-assistant-individual/raw/HEAD/images/NL-chat.png" height=200 alt="watsonx Code Assistant Individual Chat">

#### Reference code

To ask questions or refine a specific file, class, function, or method in your workspace, you can use _code references_. These references provide important context for the LLM, and can help to increase the accuracy of the answer.

1. Type the `@` sign as part of your chat message.
2. A screen pops up, and shows all files, classes, and methods from your workspace.
3. Type the characters of the file, class, or method name that you want to reference. The list filters automatically.
4. Select the reference.

<img src="https://github.com/ibm-granite/watsonx-code-assistant-individual/raw/HEAD/images/references.gif" height=200 alt="watsonx Code Assistant Individual code references">

Watsonx Code Assistant Individual sends the contents of the reference automatically to the model as part of your message.

Chat message examples:

| Use case | Example message |
| --- | --- |
| Generate a function based on an existing function | _Create a method `send_translate_message` that is similar to @send_code_explanation_message_ |
| Generate a unit test that follows existing unit tests | _Create a unit test for @getName that is similar to the unit tests in @testLoadTablesChildConnectionReceiverJob.h_ |
| Enhance existing functions | _Add error handling and log statements to @format_documents_ |
| Enhance existing functions | _Update @setEmail with error handling for null strings_ |
| Explain code | _What does @main.py do_ |
| Explain code | _Explain the logic of @send_invoice_ |
| Generate documentation for functions and classes | _Add javadoc to @Customer_ |

##### References: Indexing your workspace

When you open a workspace folder, watsonx Code Assistant Individual creates an index of these items in memory so you can reference these files and functions in the chat. The IDE also indexes files that you add or change during your Visual Studio Code session. The index contains up to 1,000 of the most recent files in 7 programming languages: C, C++, Go, Java, JavaScript, Python, and TypeScript.

#### Chat conversations

Each chat message is part of a chat conversation. We highly recommend keeping conversations focused around a specific subject or task. Create a new chat conversation to get more relevant results for your questions when you switch your context, for example, to another programming language, another project in your workspace, or a different programming task.

To create a new chat conversation:

   1. Open the menu at the top of the chat.
   2. Select **New Chat**.

To switch between chat conversations:

   1. Open the menu at the top of the chat.
   2. Select **Chat Sessions**.
   3. Select the conversation.

To delete a chat conversation:

   1. Open the menu at the top of the chat.
   2. Select **Chat Sessions**.
   3. Select the menu on the right of the conversation.
   4. Click **Delete**.

To rename a chat conversation:

   1. Open the menu at the top of the chat.
   2. Select **Chat Sessions**.
   3. Select the menu on the right of the conversation.
   4. Click **Rename**.

<img src="https://github.com/ibm-granite/watsonx-code-assistant-individual/raw/HEAD/images/Chat-Options.png" height=200 alt="watsonx Code Assistant Individual chat options">

#### Writing effective chat messages

- Write your chat messages in English.
- Start with a simple and clear instruction.
- Use subsequent chat messages to refine the code output of the model.
- Be as specific and detailed as you can for each step. You can gradually enhance the created code through subsequent chat messages.
- Use file and method references in your message to provide relevant context. For example, if you want the model to create a method that is similar to another method, add _"similar to `@<method>`"_ to your message.
- If you find the answers become less relevant, or if you start a new task, create a new chat conversation, and work from there. It is better to have many short chat conversations, each with a specific context, rather than one large conversation that might confuse the model with different and unrelated chat messages.

Watsonx Code Assistant Individual and the Granite code models are created to answer questions that are related to code, general programming, and software engineering. While the IDE doesn’t restrict your questions or prompts, the Granite code models are not designed for language tasks. Any such use is at your own risk, and results can be unreliable so validate all output independently and consider deploying a Hate Abuse Profanity (HAP) filter.
